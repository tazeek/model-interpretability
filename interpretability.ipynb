{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e706e06-f051-40a9-b04f-c73844a8037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import scipy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5fc087-9718-4368-8b33-5510ee106895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is for non-deep learning approaches\n",
    "\n",
    "# Prepare everything before interpretability\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Transform the features (before it enters the model)\n",
    "\n",
    "# Create the model and the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2baeb9a-821a-49e8-9059-294e5c42cfba",
   "metadata": {},
   "source": [
    "# LIME\n",
    "\n",
    "This section is for intrepreting models, using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6381446-4f80-4721-8595-a78577a23970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading\n",
    "model_name = \"nateraw/bert-base-uncased-emotion\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, do_lower_case=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, output_hidden_states=True)\n",
    "labels = sorted(model.config.label2id, key=model.config.label2id.get)\n",
    "\n",
    "class_names = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad0171ca-2a62-4213-a869-57f90be7c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
      "[0.01966359 0.18398522 0.01423227 0.6661366  0.0985421  0.01744025]\n"
     ]
    }
   ],
   "source": [
    "def get_proba_scores(x):\n",
    "\n",
    "    # Tokenize\n",
    "    tv = torch.tensor(\n",
    "        [\n",
    "            tokenizer.encode(v, padding=\"max_length\", max_length=128, truncation=True)\n",
    "            for v in x\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = (tv != 0).type(torch.int64)\n",
    "\n",
    "    # Get the output logits and convert to scores\n",
    "    outputs = model(tv, attention_mask=attention_mask)[0].detach().numpy()\n",
    "    scores = (np.exp(outputs).T / np.exp(outputs).sum(-1)).T\n",
    "    print(scores[0])\n",
    "    return None\n",
    "    val = sp.special.logit(scores)\n",
    "\n",
    "    return val\n",
    "\n",
    "sample = ['I really did not like what I saw and I hated every moment of it.']\n",
    "print(labels)\n",
    "get_proba_scores(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f66e046-0483-44f3-b648-6b318e5898f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5818, -0.3155, -0.1592,  ..., -0.3976,  0.1447,  0.0584],\n",
      "         [-0.4463, -0.6126, -0.5334,  ..., -0.3823,  0.5842, -0.0931],\n",
      "         [-1.2740, -0.4262,  0.5292,  ..., -0.8582,  0.1954, -0.9494],\n",
      "         ...,\n",
      "         [ 0.2317, -0.3000,  0.3801,  ..., -0.5091,  0.2487,  0.1577],\n",
      "         [ 0.0126, -0.6731,  0.2026,  ..., -0.4416,  0.2002,  0.1120],\n",
      "         [ 0.6529,  0.0146, -0.2628,  ...,  0.1625, -0.3500, -0.3292]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.0001e-01, -3.9651e-01, -8.2179e-01,  7.6373e-01,  5.4891e-01,\n",
      "         -1.5884e-01,  8.9067e-01,  2.4533e-01, -6.6508e-01, -9.9995e-01,\n",
      "         -4.6959e-01,  9.0916e-01,  9.6788e-01,  4.3327e-01,  8.8402e-01,\n",
      "         -7.8692e-01, -6.5600e-01, -5.1406e-01,  3.3625e-01, -6.8360e-01,\n",
      "          7.1572e-01,  9.9992e-01,  6.2722e-02,  4.1407e-01,  4.3372e-01,\n",
      "          9.6215e-01, -7.4032e-01,  9.0022e-01,  9.3716e-01,  6.5604e-01,\n",
      "         -7.0619e-01,  4.1921e-01, -9.8085e-01, -2.6466e-01, -8.8389e-01,\n",
      "         -9.8659e-01,  3.2603e-01, -6.1358e-01,  1.2584e-01, -5.1829e-02,\n",
      "         -8.8333e-01,  3.5678e-01,  9.9987e-01, -1.0198e-01,  3.9399e-01,\n",
      "         -2.9428e-01, -1.0000e+00,  2.6905e-01, -8.5591e-01,  8.2614e-01,\n",
      "          8.1319e-01,  8.1433e-01,  2.2139e-01,  5.5457e-01,  5.2245e-01,\n",
      "          1.4603e-02, -2.3326e-02,  2.3135e-01, -2.2632e-01, -4.6525e-01,\n",
      "         -6.0273e-01,  4.5104e-01, -7.6084e-01, -8.3150e-01,  8.2813e-01,\n",
      "          5.0431e-01, -3.8451e-01, -4.5043e-01, -1.6004e-01, -3.2423e-02,\n",
      "          8.5243e-01,  2.4315e-01, -1.0407e-02, -7.2989e-01,  5.2187e-01,\n",
      "          2.6541e-01, -5.9140e-01,  1.0000e+00, -7.7451e-01, -9.5876e-01,\n",
      "          8.1714e-01,  6.3209e-01,  5.3694e-01, -2.4774e-01,  1.3410e-01,\n",
      "         -1.0000e+00,  5.2092e-01, -2.0310e-01, -9.8055e-01,  2.3633e-01,\n",
      "          6.0007e-01, -2.3162e-01,  2.4555e-01,  6.1055e-01, -4.4914e-01,\n",
      "         -4.6550e-01, -4.1696e-01, -7.2113e-01, -4.0072e-01, -4.8024e-01,\n",
      "          1.7847e-01, -3.7203e-01, -2.7307e-01, -3.4262e-01,  2.2273e-01,\n",
      "         -5.5437e-01, -5.5612e-01,  4.5316e-01,  1.4471e-01,  6.6787e-01,\n",
      "          4.0946e-01, -3.1321e-01,  4.9445e-01, -9.4676e-01,  7.2042e-01,\n",
      "         -3.0482e-01, -9.8355e-01, -5.7747e-01, -9.8435e-01,  6.8535e-01,\n",
      "         -3.5982e-01, -2.1436e-01,  9.0571e-01, -2.8454e-02,  3.7158e-01,\n",
      "         -4.9437e-02, -8.9030e-01, -1.0000e+00, -7.5903e-01, -5.3445e-01,\n",
      "         -1.0403e-01, -3.8237e-01, -9.5907e-01, -9.3523e-01,  5.0100e-01,\n",
      "          9.3765e-01,  1.9618e-01,  9.9935e-01, -4.1492e-01,  9.1122e-01,\n",
      "         -3.4779e-01, -5.8996e-01,  6.4864e-01, -4.8930e-01,  7.0435e-01,\n",
      "          5.3304e-01, -5.5251e-01,  2.7556e-01, -6.6096e-01,  2.8231e-01,\n",
      "         -7.0802e-01, -3.3290e-01, -6.7474e-01, -8.6275e-01, -3.0165e-01,\n",
      "          9.3810e-01, -4.2198e-01, -8.4780e-01,  1.6167e-04, -3.3451e-01,\n",
      "         -5.9466e-01,  8.0879e-01,  6.5124e-01,  4.9803e-01, -2.3925e-01,\n",
      "          3.6905e-01,  2.9749e-01,  5.5849e-01, -8.1491e-01, -2.7537e-01,\n",
      "          4.2008e-01, -4.0350e-01, -7.9119e-01, -9.7745e-01, -5.0106e-01,\n",
      "          3.4774e-01,  9.7943e-01,  7.3430e-01,  2.0794e-01,  7.5749e-01,\n",
      "         -3.2688e-01,  6.9915e-01, -9.4180e-01,  9.6979e-01, -2.7708e-01,\n",
      "          3.3034e-01, -4.4776e-02,  3.3575e-01, -8.1066e-01, -1.9242e-01,\n",
      "          9.1979e-01, -5.8373e-01, -7.6243e-01, -1.3282e-01, -3.7517e-01,\n",
      "         -4.3847e-01, -7.2119e-01,  6.4438e-01, -3.1275e-01, -3.0463e-01,\n",
      "         -7.1364e-02,  8.5644e-01,  9.8255e-01,  7.7850e-01,  2.4869e-01,\n",
      "          6.3778e-01, -8.4857e-01, -4.4990e-01,  3.4005e-01,  2.3054e-01,\n",
      "          1.8830e-01,  9.8801e-01, -3.1513e-01, -1.1156e-01, -8.7299e-01,\n",
      "         -9.7813e-01,  4.0100e-02, -8.7578e-01, -2.2256e-01, -7.0408e-01,\n",
      "          5.4486e-01,  9.9180e-02,  3.9101e-01,  2.8515e-01, -9.8246e-01,\n",
      "         -6.8817e-01,  3.8869e-01, -2.5254e-01,  4.5588e-01, -2.1938e-01,\n",
      "          1.3027e-01,  9.1213e-01, -5.8828e-01,  8.0713e-01,  8.9149e-01,\n",
      "         -7.8185e-01, -7.8622e-01,  8.0494e-01, -3.9818e-01,  8.5506e-01,\n",
      "         -6.2370e-01,  9.7673e-01,  8.9299e-01,  7.2363e-01, -8.9093e-01,\n",
      "         -4.7439e-01, -8.9684e-01, -5.4639e-01, -8.9137e-02, -2.3645e-01,\n",
      "          7.2725e-01,  5.6331e-01,  3.4254e-01,  5.1396e-01, -6.6802e-01,\n",
      "          9.9667e-01,  4.5115e-01, -9.4379e-01, -2.7805e-02, -7.8060e-02,\n",
      "         -9.8254e-01,  7.5687e-01,  3.6794e-01,  1.7605e-01, -4.1530e-01,\n",
      "         -7.3750e-01, -9.4065e-01,  8.8577e-01,  1.3436e-01,  9.8379e-01,\n",
      "         -5.0426e-02, -9.2955e-01, -6.4495e-01, -8.6628e-01, -1.7732e-02,\n",
      "         -3.6141e-01, -2.8387e-01,  8.1297e-02, -9.1328e-01,  4.8237e-01,\n",
      "          4.5199e-01,  4.4915e-01, -5.3199e-01,  9.9715e-01,  1.0000e+00,\n",
      "          9.4502e-01,  7.9818e-01,  8.0992e-01, -9.9916e-01, -1.5429e-01,\n",
      "          9.9998e-01, -9.7945e-01, -1.0000e+00, -9.2174e-01, -7.8967e-01,\n",
      "          3.3511e-01, -1.0000e+00, -1.7747e-01, -1.2549e-01, -8.3219e-01,\n",
      "          7.1295e-01,  9.7108e-01,  9.8544e-01, -1.0000e+00,  7.5113e-01,\n",
      "          9.4765e-01, -5.6472e-01,  9.2668e-01, -4.5026e-01,  9.6543e-01,\n",
      "          3.6078e-01,  5.0494e-01, -1.4905e-01,  3.2304e-01, -8.4908e-01,\n",
      "         -8.8100e-01, -3.7716e-01, -3.4966e-01,  9.9551e-01,  3.4538e-01,\n",
      "         -8.7016e-01, -8.3376e-01,  2.9599e-01, -2.4969e-01, -1.0880e-03,\n",
      "         -9.4388e-01, -2.1441e-01,  6.3200e-01,  7.3682e-01,  2.0005e-01,\n",
      "          1.9326e-01, -6.1989e-01,  3.4113e-01, -1.2281e-01,  5.4154e-02,\n",
      "          5.8175e-01, -8.2903e-01, -7.0300e-01, -4.3907e-01, -1.0965e-02,\n",
      "         -4.2263e-01, -9.4698e-01,  9.3041e-01, -4.3662e-01,  8.3722e-01,\n",
      "          1.0000e+00, -6.4364e-02, -7.7302e-01,  6.3763e-01,  3.4437e-01,\n",
      "         -1.9443e-01,  1.0000e+00,  7.0446e-01, -9.6589e-01, -5.3893e-01,\n",
      "          4.2794e-01, -5.4887e-01, -5.0892e-01,  9.9839e-01, -3.6236e-01,\n",
      "         -5.8392e-01, -4.2483e-01,  9.6867e-01, -9.8710e-01,  9.7833e-01,\n",
      "         -8.5688e-01, -9.5291e-01,  9.2575e-01,  8.5417e-01, -5.3535e-01,\n",
      "         -6.2540e-01,  2.3184e-01, -5.0266e-01,  4.3008e-01, -9.4286e-01,\n",
      "          8.1637e-01,  4.9257e-01, -8.1452e-02,  8.1954e-01, -9.2645e-01,\n",
      "         -5.4967e-01,  4.3824e-01, -7.3365e-01, -3.8723e-01,  7.3119e-01,\n",
      "          5.6543e-01, -3.0593e-01,  2.3550e-01, -3.5755e-01,  5.3074e-02,\n",
      "         -9.5555e-01,  4.6141e-01,  1.0000e+00, -3.2313e-01,  4.3473e-01,\n",
      "         -4.5136e-01,  1.8302e-02, -4.7622e-02,  4.7659e-01,  5.9751e-01,\n",
      "         -2.9422e-01, -6.7721e-01,  4.3800e-01, -9.1413e-01, -9.8498e-01,\n",
      "          6.1466e-01,  3.5535e-01, -1.3720e-01,  9.9999e-01,  5.1321e-01,\n",
      "          2.4087e-01,  1.9103e-01,  9.7179e-01,  7.3064e-02,  3.6057e-01,\n",
      "          7.3808e-01,  9.5620e-01, -4.0860e-01,  5.6974e-01,  8.5370e-01,\n",
      "         -7.2362e-01, -2.5303e-01, -6.3075e-01,  9.3694e-02, -8.8875e-01,\n",
      "         -1.4804e-02, -9.4479e-01,  9.4616e-01,  8.8826e-01,  3.4875e-01,\n",
      "          2.6637e-01,  7.7430e-01,  1.0000e+00, -3.3888e-01,  6.4759e-01,\n",
      "         -7.0813e-01,  7.4618e-01, -9.9866e-01, -6.7034e-01, -3.0529e-01,\n",
      "         -1.1467e-01, -5.9314e-01, -2.9470e-01,  2.4786e-01, -9.5181e-01,\n",
      "          7.3330e-01,  3.2263e-01, -9.7240e-01, -9.8602e-01, -2.4658e-01,\n",
      "          9.4014e-01,  3.0375e-02, -9.6679e-01, -7.8488e-01, -5.4688e-01,\n",
      "          5.4368e-01, -3.3769e-01, -9.3477e-01, -1.2971e-01, -3.5561e-01,\n",
      "          4.4518e-01, -2.5191e-01,  5.8691e-01,  6.7005e-01,  7.8167e-01,\n",
      "         -6.8400e-01, -4.0570e-01, -3.1211e-01, -8.5107e-01,  8.3808e-01,\n",
      "         -7.9239e-01, -8.5181e-01, -1.2032e-01,  1.0000e+00, -3.6763e-01,\n",
      "          7.6521e-01,  5.7161e-01,  6.9153e-01, -1.8898e-01,  1.7027e-01,\n",
      "          8.0944e-01,  2.8479e-01, -4.9643e-01, -8.2455e-01, -8.4516e-01,\n",
      "         -3.4671e-01,  6.6206e-01,  2.8695e-01,  4.2810e-01,  7.5167e-01,\n",
      "          6.4012e-01,  3.6568e-01, -1.8782e-01,  1.1143e-01,  9.9933e-01,\n",
      "         -2.0377e-01, -1.7878e-01, -3.6554e-01, -1.4092e-01, -3.1623e-01,\n",
      "         -6.0639e-01,  1.0000e+00,  3.8981e-01,  3.9029e-01, -9.9034e-01,\n",
      "         -7.7177e-01, -9.2903e-01,  1.0000e+00,  8.2504e-01, -7.1628e-01,\n",
      "          6.0623e-01,  6.5841e-01, -2.1509e-01,  7.8423e-01, -2.8010e-01,\n",
      "         -3.5274e-01,  3.2119e-01,  1.7178e-01,  9.1704e-01, -4.9963e-01,\n",
      "         -9.6366e-01, -6.5599e-01,  4.9899e-01, -9.0261e-01,  9.9951e-01,\n",
      "         -6.5126e-01, -2.4104e-01, -2.5531e-01, -3.6554e-01,  8.2891e-01,\n",
      "          1.1345e-01, -9.5653e-01, -4.0855e-01,  3.4884e-01,  9.4415e-01,\n",
      "          2.1729e-01, -6.3965e-01, -9.4929e-01,  7.1621e-01,  7.9109e-01,\n",
      "         -7.8971e-01, -8.8316e-01,  9.3493e-01, -9.6003e-01,  6.0310e-01,\n",
      "          1.0000e+00,  4.0023e-01,  4.0711e-01,  2.8719e-01, -4.0758e-01,\n",
      "          4.0264e-01, -2.9257e-01,  7.6789e-01, -8.9514e-01, -3.8846e-01,\n",
      "         -2.4733e-01,  4.4047e-01, -1.8719e-01, -2.9856e-01,  7.0208e-01,\n",
      "          1.0329e-01, -5.5279e-01, -5.8992e-01, -8.9487e-02,  5.5782e-01,\n",
      "          8.4435e-01, -3.2118e-01, -2.3144e-01,  2.5432e-01, -1.2966e-01,\n",
      "         -8.4940e-01, -4.2166e-01, -3.0214e-01, -9.9996e-01,  6.3616e-01,\n",
      "         -1.0000e+00,  2.9967e-01, -5.9327e-03, -1.5601e-01,  8.2509e-01,\n",
      "          2.5760e-01,  5.3683e-01, -7.4426e-01, -7.8386e-01,  4.5548e-01,\n",
      "          7.4462e-01, -2.6414e-01, -5.8638e-01, -5.4635e-01,  2.8817e-01,\n",
      "         -1.3035e-01,  1.5413e-01, -6.2402e-01,  6.6606e-01, -2.6815e-01,\n",
      "          1.0000e+00,  1.7251e-01, -7.6277e-01, -9.6623e-01,  2.7043e-01,\n",
      "         -3.3335e-01,  1.0000e+00, -9.0413e-01, -9.5074e-01,  4.0611e-01,\n",
      "         -8.0488e-01, -7.3410e-01,  4.7318e-01,  9.5916e-02, -7.0657e-01,\n",
      "         -8.9138e-01,  8.5242e-01,  9.3418e-01, -6.5117e-01,  5.2836e-01,\n",
      "         -3.2677e-01, -6.4793e-01,  1.8349e-01,  7.4732e-01,  9.8216e-01,\n",
      "          4.6278e-01,  8.6984e-01,  4.1182e-01, -4.1178e-02,  9.2250e-01,\n",
      "          2.5737e-01,  4.7544e-01,  1.3918e-01,  1.0000e+00,  4.1561e-01,\n",
      "         -9.2867e-01, -3.5007e-02, -9.6017e-01, -3.5307e-01, -9.2020e-01,\n",
      "          3.9020e-01,  2.8277e-01,  8.6073e-01, -3.4200e-01,  9.3140e-01,\n",
      "         -6.9912e-01,  2.0300e-01, -5.9933e-01, -4.5567e-01,  3.9893e-01,\n",
      "         -9.3173e-01, -9.7676e-01, -9.7822e-01,  6.3113e-01, -5.2141e-01,\n",
      "         -5.7975e-03,  3.1553e-01,  1.2181e-01,  3.6440e-01,  4.7490e-01,\n",
      "         -1.0000e+00,  9.3164e-01,  4.7655e-01,  6.8774e-01,  9.4671e-01,\n",
      "          5.9615e-01,  6.2755e-01,  2.1771e-01, -9.8156e-01, -9.6803e-01,\n",
      "         -3.9123e-01, -3.7017e-01,  8.0243e-01,  6.9966e-01,  7.8482e-01,\n",
      "          6.1326e-01, -4.7539e-01, -3.2179e-01, -4.2646e-01,  6.0465e-02,\n",
      "         -9.8801e-01,  4.1162e-01, -5.0089e-01, -9.5103e-01,  9.2703e-01,\n",
      "         -2.0000e-01, -1.5712e-01,  2.4142e-01, -7.7463e-01,  9.4666e-01,\n",
      "          6.8258e-01,  5.1668e-01,  7.3060e-02,  3.9073e-01,  8.1973e-01,\n",
      "          9.4127e-01,  9.8429e-01, -6.8275e-01,  7.5425e-01, -4.9194e-01,\n",
      "          4.7847e-01,  3.2418e-01, -9.0354e-01,  1.4062e-01,  3.7069e-01,\n",
      "         -3.3896e-01,  2.6807e-01, -3.4647e-01, -9.3336e-01,  6.3633e-01,\n",
      "         -2.3477e-01,  5.3543e-01, -3.2699e-01, -2.1031e-02, -4.3651e-01,\n",
      "         -2.8069e-01, -6.7081e-01, -8.2414e-01,  6.6572e-01,  3.5906e-01,\n",
      "          9.0247e-01,  6.3554e-01,  1.2114e-03, -8.1868e-01, -4.2061e-01,\n",
      "         -4.8949e-01, -9.1744e-01,  9.1500e-01, -1.7027e-01, -1.0176e-01,\n",
      "          5.9244e-01,  1.1436e-01,  7.5562e-01,  1.2140e-01, -5.3602e-01,\n",
      "         -3.4550e-01, -6.3152e-01,  7.9299e-01, -5.9607e-01, -5.6676e-01,\n",
      "         -7.3870e-01,  6.9583e-01,  3.2548e-01,  9.9992e-01, -6.0018e-01,\n",
      "         -8.2369e-01, -5.5237e-01, -4.3849e-01,  3.7276e-01, -5.6197e-01,\n",
      "         -1.0000e+00,  3.9216e-01, -5.1643e-01,  4.5112e-01, -5.2954e-01,\n",
      "          6.1895e-01, -6.7235e-01, -9.5640e-01, -3.0334e-01,  1.9813e-01,\n",
      "          6.5822e-01, -5.4934e-01, -7.6598e-01,  5.9452e-01, -3.9711e-01,\n",
      "          9.5194e-01,  8.2006e-01, -6.9959e-01,  2.6878e-01,  5.9812e-01,\n",
      "         -7.1837e-01, -5.0401e-01,  7.8494e-01]], grad_fn=<TanhBackward0>), hidden_states=(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.2180, -0.3051, -0.7017,  ...,  0.3223,  0.1959,  0.1303],\n",
      "         [-0.2854, -0.2123,  0.0105,  ...,  0.0843,  0.6783, -0.7823],\n",
      "         ...,\n",
      "         [-0.9384,  0.0608,  0.6821,  ..., -0.1381,  0.6612,  0.0058],\n",
      "         [ 1.2905,  0.0785, -0.3211,  ...,  0.3380,  0.4456,  0.7700],\n",
      "         [-0.5461,  0.1114,  0.1646,  ..., -0.2541,  0.0671,  0.0437]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1186,  0.0452,  0.0277,  ...,  0.0067,  0.0592,  0.0690],\n",
      "         [ 0.0714, -0.6828, -0.6091,  ..., -0.1412,  0.3133, -0.0066],\n",
      "         [-0.3310, -0.0719,  0.3817,  ..., -0.0497,  0.5383, -0.3219],\n",
      "         ...,\n",
      "         [-0.6802,  0.0707,  1.0701,  ..., -0.1263,  1.3167,  0.0955],\n",
      "         [ 1.7681, -0.5946,  0.3874,  ...,  0.6732,  0.3160,  1.1818],\n",
      "         [-0.4039,  0.1499,  0.1781,  ..., -0.2879,  0.5442,  0.1609]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0309, -0.2207, -0.0623,  ...,  0.0205,  0.0968,  0.0360],\n",
      "         [ 0.1572, -0.7779, -0.3567,  ..., -0.1475, -0.3543, -0.5754],\n",
      "         [-0.3193,  0.3603,  0.3952,  ..., -0.3352,  0.2287, -0.3149],\n",
      "         ...,\n",
      "         [-0.6223,  0.7066,  1.4529,  ..., -0.3056,  1.4668,  0.2485],\n",
      "         [ 2.4598, -0.1780,  0.4579,  ...,  0.8865,  0.0987,  1.6391],\n",
      "         [-0.2810,  0.0235,  0.2425,  ..., -0.0339,  0.3519, -0.0942]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0696, -0.2349, -0.0850,  ...,  0.0400,  0.1233,  0.1799],\n",
      "         [ 0.4458, -0.7031,  0.1179,  ..., -0.2672, -0.2129, -0.6318],\n",
      "         [ 0.0074,  0.1156,  0.5679,  ..., -0.5497,  0.4444, -0.3190],\n",
      "         ...,\n",
      "         [-0.1574,  0.1045,  1.4947,  ...,  0.9968,  1.3276,  0.4980],\n",
      "         [ 2.3513, -0.1452,  0.3346,  ...,  0.3806, -0.4633,  1.2659],\n",
      "         [-0.0619, -0.1014,  0.1138,  ...,  0.0260,  0.0557, -0.0266]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2201, -0.3801, -0.2817,  ...,  0.1166,  0.1220,  0.3986],\n",
      "         [ 0.5803, -0.7670, -0.0460,  ...,  0.3503, -0.4328, -0.8178],\n",
      "         [ 0.0615,  0.1449,  0.4759,  ..., -0.6639,  0.5730, -0.4547],\n",
      "         ...,\n",
      "         [-0.1357,  0.1235,  1.3315,  ...,  0.4773,  1.1258,  0.7307],\n",
      "         [ 1.9291, -0.1111,  0.1892,  ..., -0.0179, -0.1895,  1.4101],\n",
      "         [-0.0201, -0.0423,  0.0143,  ...,  0.0117,  0.0473, -0.0362]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2639, -0.1933, -0.1105,  ..., -0.1782, -0.1277,  0.5379],\n",
      "         [ 0.3837, -0.9070, -0.3473,  ...,  0.1769,  0.0262, -0.4457],\n",
      "         [ 0.0082, -0.1313,  0.8806,  ..., -0.7908,  0.8506, -0.5107],\n",
      "         ...,\n",
      "         [-0.0720,  0.0712,  1.7685,  ...,  0.2028,  1.1198,  0.5374],\n",
      "         [ 1.7496, -0.5645,  0.3461,  ...,  0.1253,  0.2536,  0.9428],\n",
      "         [-0.0144, -0.0265,  0.0274,  ...,  0.0084,  0.0049, -0.0384]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3989, -0.3938, -0.1700,  ..., -0.3794,  0.1045,  0.8431],\n",
      "         [ 0.1186, -0.8677, -0.0614,  ...,  0.1183,  0.0249, -0.5071],\n",
      "         [-0.1105, -0.0483,  1.4552,  ..., -0.7470,  0.7469, -0.3668],\n",
      "         ...,\n",
      "         [-0.0172, -0.1922,  2.1888,  ...,  0.2870,  1.4069,  0.4330],\n",
      "         [ 1.4148, -0.6119,  0.3963,  ...,  0.1949,  0.0709,  0.9205],\n",
      "         [ 0.0124, -0.0218, -0.0140,  ...,  0.0039, -0.0197, -0.0321]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-5.1273e-01, -5.2517e-01, -1.4366e-01,  ..., -5.9091e-01,\n",
      "           1.7495e-02,  9.1024e-01],\n",
      "         [-2.3006e-02, -8.1483e-01,  3.4526e-01,  ...,  7.6931e-02,\n",
      "           6.2854e-01, -3.7194e-01],\n",
      "         [-5.7029e-01,  1.6483e-01,  1.3735e+00,  ..., -7.0876e-01,\n",
      "           1.5547e+00, -3.3965e-01],\n",
      "         ...,\n",
      "         [ 1.0933e-01, -4.3976e-01,  1.7371e+00,  ...,  3.9712e-01,\n",
      "           1.6032e+00,  6.4118e-01],\n",
      "         [ 9.8453e-01, -6.5925e-01,  4.3247e-01,  ..., -9.2623e-02,\n",
      "          -1.3092e-02,  1.3215e+00],\n",
      "         [-5.3326e-03, -3.0575e-02,  2.9884e-04,  ..., -1.7762e-02,\n",
      "           1.2829e-02, -4.7962e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.6814, -0.5597, -0.4842,  ..., -0.8511,  0.1295,  0.7948],\n",
      "         [-0.2896, -0.8336,  0.4003,  ..., -0.3343,  0.6178, -0.3408],\n",
      "         [-1.0156,  0.1192,  0.6957,  ..., -0.8334,  1.3165, -0.1444],\n",
      "         ...,\n",
      "         [ 0.2151, -0.7647,  1.5809,  ...,  0.1452,  1.9233,  0.3202],\n",
      "         [ 0.8577, -0.9294,  0.0709,  ..., -0.0215,  0.3526,  1.5031],\n",
      "         [-0.0026, -0.0186,  0.0291,  ..., -0.0511, -0.0269, -0.0631]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.1265, -0.7567, -0.2050,  ..., -0.1902,  0.2713,  0.5149],\n",
      "         [-0.6719, -0.8754,  0.8857,  ..., -0.5259,  0.5490, -0.4548],\n",
      "         [-1.2418, -0.1453,  1.1342,  ..., -0.9610,  0.9303, -0.7056],\n",
      "         ...,\n",
      "         [ 0.3187, -0.7068,  1.2446,  ..., -0.0283,  1.4893,  0.4711],\n",
      "         [ 0.7506, -0.8533,  0.4283,  ..., -0.4094,  0.5852,  1.0458],\n",
      "         [-0.0037, -0.0057, -0.0021,  ..., -0.0413, -0.0618, -0.0240]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.3208e+00, -1.1711e+00, -5.0908e-01,  ..., -6.1927e-01,\n",
      "          -2.7114e-01, -1.0278e-01],\n",
      "         [-7.5151e-01, -1.3412e+00,  4.9531e-01,  ..., -1.1886e+00,\n",
      "           2.1979e-01, -8.5687e-01],\n",
      "         [-1.2736e+00, -3.7030e-01,  1.0983e+00,  ..., -1.5663e+00,\n",
      "           7.8268e-01, -1.1094e+00],\n",
      "         ...,\n",
      "         [ 2.7017e-01, -7.3935e-01,  1.0351e+00,  ..., -3.8351e-01,\n",
      "           1.5405e+00,  4.6753e-01],\n",
      "         [ 3.9022e-01, -1.0788e+00,  5.0690e-01,  ..., -8.5089e-01,\n",
      "           7.6426e-01,  3.2768e-01],\n",
      "         [-1.0703e-02, -6.2119e-04, -6.8563e-02,  ..., -3.0000e-02,\n",
      "          -4.9139e-02, -2.0359e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.0117, -0.7186, -0.4436,  ..., -0.7160, -0.2879, -0.3057],\n",
      "         [-0.5419, -1.1533,  0.0422,  ..., -1.4723,  0.0285, -0.9391],\n",
      "         [-1.6031, -0.5101,  0.7160,  ..., -1.4430,  0.5954, -0.9970],\n",
      "         ...,\n",
      "         [ 0.4616, -0.7225,  0.6964,  ..., -0.5931,  0.8775,  0.2681],\n",
      "         [ 0.2515, -0.9225,  0.3071,  ..., -0.8988,  0.5182,  0.2916],\n",
      "         [ 0.0402,  0.0461, -0.0362,  ..., -0.0078, -0.0448,  0.0122]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5818, -0.3155, -0.1592,  ..., -0.3976,  0.1447,  0.0584],\n",
      "         [-0.4463, -0.6126, -0.5334,  ..., -0.3823,  0.5842, -0.0931],\n",
      "         [-1.2740, -0.4262,  0.5292,  ..., -0.8582,  0.1954, -0.9494],\n",
      "         ...,\n",
      "         [ 0.2317, -0.3000,  0.3801,  ..., -0.5091,  0.2487,  0.1577],\n",
      "         [ 0.0126, -0.6731,  0.2026,  ..., -0.4416,  0.2002,  0.1120],\n",
      "         [ 0.6529,  0.0146, -0.2628,  ...,  0.1625, -0.3500, -0.3292]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Create the function to fetch the probabiltiy scores\n",
    "def get_probability_scores(text):\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Get output from model\n",
    "    output = model(**tokenized)\n",
    "    return None\n",
    "\n",
    "    # Get the probabilities\n",
    "    ...\n",
    "\n",
    "sample_text = 'surprising increase in revenue in spite of decrease in market share'\n",
    "get_probability_scores(sample_text)\n",
    "\n",
    "# Create the lime text explainer object\n",
    "\n",
    "# Create"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6fd822-3c00-4017-b1c3-185dfea33d12",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "\n",
    "This section is for interpreting models using SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4e2b2-75f5-4b73-ad62-606bc82c3e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
